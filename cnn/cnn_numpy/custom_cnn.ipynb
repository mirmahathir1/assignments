{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist,cifar10\n",
    "from tqdm import tqdm\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [],
   "source": [
    "def convert_categorical2one_hot(y: np.array) -> np.array:\n",
    "    one_hot_matrix = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot_matrix[np.arange(y.size), y] = 1\n",
    "    return one_hot_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, layers):\n",
    "        for layer in layers:\n",
    "            weights_biases = layer.get_weights()\n",
    "            gradients_weights_biases = layer.get_gradients()\n",
    "            if weights_biases is None or gradients_weights_biases is None:\n",
    "                continue\n",
    "\n",
    "            (weights, biases) = weights_biases\n",
    "            (derivative_weights, derivative_biases) = gradients_weights_biases\n",
    "            layer.set_weights(\n",
    "                weights=weights - self.learning_rate * derivative_weights,\n",
    "                biases=biases - self.learning_rate * derivative_biases\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [],
   "source": [
    "def generate_batches(x, y, batch_size):\n",
    "    for i in range(0, x.shape[0], batch_size):\n",
    "        yield (x.take(indices=range(\n",
    "            i, min(i + batch_size, x.shape[0])), axis=0),\n",
    "               y.take(indices=range(\n",
    "                   i, min(i + batch_size, y.shape[0])), axis=0)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [],
   "source": [
    "def probability_to_one_hot(probabilities):\n",
    "    class_index = np.argmax(probabilities, axis=1)\n",
    "    one_hot_matrix = np.zeros_like(probabilities)\n",
    "    one_hot_matrix[np.arange(probabilities.shape[0]), class_index] = 1\n",
    "    return one_hot_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [],
   "source": [
    "def softmax_accuracy(y_hat, y):\n",
    "    y_hat = probability_to_one_hot(y_hat)\n",
    "    return (y_hat == y).all(axis=1).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(y_hat, y, epsilon=1e-20):\n",
    "    n = y_hat.shape[0]\n",
    "    return - np.sum(y * np.log(np.clip(y_hat, epsilon, 1.))) / n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "outputs": [],
   "source": [
    "class SequentialModel:\n",
    "    def __init__(self, layers, ):\n",
    "        self.layers = layers\n",
    "        self.optimizer = GradientDescent(learning_rate=0.01)\n",
    "\n",
    "        self.training_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "    def train(self, x_train, y_train, x_validation, y_validation, epoch_count, batch_size):\n",
    "        for epoch in range(epoch_count):\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            y_hat = np.zeros_like(y_train)\n",
    "            for index, (x_batch, y_batch) in enumerate(tqdm(generate_batches(x=x_train, y=y_train, batch_size=batch_size),total=int(x_train.shape[0]/batch_size))):\n",
    "                y_hat_batch = self.forward(x=x_batch)\n",
    "                activation = y_hat_batch - y_batch\n",
    "                self.backward(activation)\n",
    "                self.update()\n",
    "                number_start = index * batch_size\n",
    "                number_end = number_start + y_hat_batch.shape[0]\n",
    "                y_hat[number_start: number_end, :] = y_hat_batch\n",
    "\n",
    "            self.training_accuracies.append(softmax_accuracy(y_hat=y_hat, y=y_train))\n",
    "            self.train_losses.append(softmax_cross_entropy(y_hat=y_hat, y=y_train))\n",
    "\n",
    "            y_hat = self.forward(x_validation)\n",
    "            validation_accuracy = softmax_accuracy(y_hat=y_hat, y=y_validation)\n",
    "            self.validation_accuracies.append(validation_accuracy)\n",
    "            validation_loss = softmax_cross_entropy(y_hat=y_hat, y=y_validation)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "            print(f\"{epoch + 1} validation loss {validation_loss} validation acc {validation_accuracy}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activation = x\n",
    "        for layer in self.layers:\n",
    "            # print(f\"forward layer {layer.__class__.__name__}\")\n",
    "            # print(f\"input dim: {activation.shape}\")\n",
    "            activation = layer.forward(a_previous=activation)\n",
    "            # print(f\"output dim: {activation.shape}\")\n",
    "        return activation\n",
    "\n",
    "    def backward(self, x):\n",
    "        activation = x\n",
    "        for layer in reversed(self.layers):\n",
    "            # print(f\"backward layer {layer.__class__.__name__}\")\n",
    "            # print(f\"input dim: {activation.shape}\")\n",
    "            activation = layer.backward(da_current=activation)\n",
    "            # print(f\"output dim: {activation.shape}\")\n",
    "\n",
    "    def update(self):\n",
    "        self.optimizer.update(layers=self.layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [],
   "source": [
    "class PoolingLayer:\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.a = None\n",
    "        self.cache = {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None\n",
    "    def forward(self, a_previous):\n",
    "        self.a = np.array(a_previous, copy=True)\n",
    "        number_example_in_batch, height_input, width_input, channel_count = a_previous.shape\n",
    "        height_pool, width_pool = self.pool_size\n",
    "        height_output = 1 + (height_input - height_pool) // self.stride\n",
    "        width_output = 1 + (width_input - width_pool) // self.stride\n",
    "        output = np.zeros((number_example_in_batch, height_output, width_output, channel_count))\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + height_pool\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + width_pool\n",
    "                a_previous_slice = a_previous[:, height_start:height_end, width_start:width_end, :]\n",
    "                self.save_mask(x=a_previous_slice, coordinates=(i, j))\n",
    "                output[:, i, j, :] = np.max(a_previous_slice, axis=(1, 2))\n",
    "        return output\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        output = np.zeros_like(self.a)\n",
    "        _, height_output, width_output, _ = da_current.shape\n",
    "        height_pool, width_pool = self.pool_size\n",
    "\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + height_pool\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + width_pool\n",
    "                output[:, height_start:height_end, width_start:width_end, :] += da_current[:, i:i + 1, j:j + 1, :] * \\\n",
    "                                                                                self.cache[(i, j)]\n",
    "        return output\n",
    "\n",
    "    def save_mask(self, x, coordinates):\n",
    "        mask = np.zeros_like(x)\n",
    "        number_example_in_batch, height, width, channel_count = x.shape\n",
    "        x = x.reshape(number_example_in_batch, height * width, channel_count)\n",
    "        index = np.argmax(x, axis=1)\n",
    "        number_index, channel_index = np.indices((number_example_in_batch, channel_count))\n",
    "        mask.reshape(number_example_in_batch, height * width, channel_count)[number_index, index, channel_index] = 1\n",
    "        self.cache[coordinates] = mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        self.shape = ()\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        self.shape = a_previous.shape\n",
    "        return np.ravel(a_previous).reshape(a_previous.shape[0], -1)\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        return da_current.reshape(self.shape)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        self.derivative_weights = None\n",
    "        self.derivative_biases = None\n",
    "        self.a_previous = None\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls, unit_count_previous_layer, unit_count_current_layer):\n",
    "        weights = np.random.randn(unit_count_current_layer, unit_count_previous_layer) * 0.1\n",
    "        biases = np.random.randn(1, unit_count_current_layer) * 0.1\n",
    "        return cls(weights=weights, biases=biases)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def get_gradients(self):\n",
    "        if self.derivative_weights is None or self.derivative_biases is None:\n",
    "            return None\n",
    "        return self.derivative_weights, self.derivative_biases\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        self.a_previous = np.array(a_previous, copy=True)\n",
    "        return np.dot(a_previous, self.weights.T) + self.biases\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        number_example_in_batch = self.a_previous.shape[0]\n",
    "        self.derivative_weights = np.dot(da_current.T, self.a_previous) / number_example_in_batch\n",
    "        self.derivative_biases = np.sum(da_current, axis=0, keepdims=True) / number_example_in_batch\n",
    "        return np.dot(da_current, self.weights)\n",
    "\n",
    "    def set_weights(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, weights, biases, padding, stride):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.derivative_weights = None\n",
    "        self.derivative_biases = None\n",
    "        self.a_previous = None\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls, filters, kernel_shape, padding, stride):\n",
    "        weights = np.random.randn(*kernel_shape, filters) * 0.1\n",
    "        biases = np.random.randn(filters) * 0.1\n",
    "        return cls(weights=weights, biases=biases, padding=padding, stride=stride)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def set_weights(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def get_gradients(self):\n",
    "        if self.derivative_weights is None or self.derivative_biases is None:\n",
    "            return None\n",
    "        return self.derivative_weights, self.derivative_biases\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        self.a_previous = np.array(a_previous, copy=True)\n",
    "        output_shape = self.calculate_output_dims(input_dims=a_previous.shape)\n",
    "        number_examples_in_batch, height_input, width_of_input, _ = a_previous.shape\n",
    "        _, height_output, width_output, _ = output_shape\n",
    "        height_filter, width_filter, _, number_filter = self.weights.shape\n",
    "        padding = np.full((4,), self.padding)\n",
    "        a_previous_padding = self.pad(array=a_previous, padding=padding)\n",
    "        output = np.zeros(output_shape)\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + height_filter\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + width_filter\n",
    "\n",
    "                output[:, i, j, :] = np.sum(\n",
    "                    a_previous_padding[:, height_start:height_end, width_start:width_end, :, np.newaxis] * self.weights[\n",
    "                                                                                                           np.newaxis,\n",
    "                                                                                                           :, :,\n",
    "                                                                                                           :],\n",
    "                    axis=(1, 2, 3))\n",
    "        return output + self.biases\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        _, height_output, width_output, _ = da_current.shape\n",
    "        number_example_in_batch, height_input, width_input, _ = self.a_previous.shape\n",
    "        height_filter, width_filter, _, _ = self.weights.shape\n",
    "        padding = np.full((4,), self.padding)\n",
    "        a_previous_padding = self.pad(array=self.a_previous, padding=padding)\n",
    "        output = np.zeros_like(a_previous_padding)\n",
    "        self.derivative_biases = da_current.sum(axis=(0, 1, 2)) / number_example_in_batch\n",
    "        self.derivative_weights = np.zeros_like(self.weights)\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + height_filter\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + width_filter\n",
    "                output[:, height_start:height_end, width_start: width_end, :] += np.sum(\n",
    "                    self.weights[np.newaxis, :, :, :, :] *\n",
    "                    da_current[:, i:i + 1, j:j + 1, np.newaxis, :],\n",
    "                    axis=4\n",
    "                )\n",
    "                self.derivative_weights += np.sum(\n",
    "                    a_previous_padding[:, height_start: height_end, width_start: width_end, :, np.newaxis] *\n",
    "                    da_current[:, i:i + 1, j:j + 1, np.newaxis, :],\n",
    "                    axis=0\n",
    "                )\n",
    "        self.derivative_weights /= number_example_in_batch\n",
    "        return output[:, padding[0]:padding[0] + height_input, padding[1]:padding[1] + width_input, :]\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(array, padding):\n",
    "        return np.pad(array=array, pad_width=((0, 0), (padding[0], padding[0]), (padding[1], padding[1]), (0, 0)),\n",
    "                      mode='constant')\n",
    "\n",
    "    def calculate_output_dims(self, input_dims):\n",
    "        print(f\"input_dims: {input_dims}\")\n",
    "        number_examples_in_batch, height_input, width_of_input, _ = input_dims\n",
    "        height_filter, width_filter, _, number_filters = self.weights.shape\n",
    "\n",
    "        height_output = (height_input - height_filter + 2 * self.padding) // self.stride + 1\n",
    "        width_output = (width_of_input - width_filter + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        return number_examples_in_batch, height_output, width_output, number_filters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        self.z = np.maximum(0, a_previous)\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        derivative_z = np.array(da_current, copy=True)\n",
    "        derivative_z[self.z <= 0] = 0\n",
    "        return derivative_z\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        exponential = np.exp(a_previous - a_previous.max(axis=1, keepdims=True))\n",
    "        self.z = exponential / np.sum(exponential, axis=1, keepdims=True)\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        return da_current\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "outputs": [],
   "source": [
    "# number of samples in the train data set\n",
    "N_TRAIN_SAMPLES = 5000\n",
    "dataset = \"cifar10\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape: (50000, 32, 32)\n",
      "trainY shape: (50000, 1)\n",
      "testX shape: (10000, 32, 32)\n",
      "testY shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = None, None, None, None\n",
    "if dataset == \"mnist\":\n",
    "    ((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "elif dataset == \"cifar10\":\n",
    "    ((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "    trainX = rgb2gray(trainX)\n",
    "    testX = rgb2gray(testX)\n",
    "\n",
    "print(\"trainX shape:\", trainX.shape)\n",
    "print(\"trainY shape:\", trainY.shape)\n",
    "print(\"testX shape:\", testX.shape)\n",
    "print(\"testY shape:\", testY.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [],
   "source": [
    "X_train = trainX[:N_TRAIN_SAMPLES, :, :]\n",
    "Y_train = trainY[:N_TRAIN_SAMPLES]\n",
    "\n",
    "test_size = int(testX.shape[0]/2)\n",
    "X_test = testX[:test_size, :, :]\n",
    "Y_test = testY[:test_size]\n",
    "\n",
    "X_valid = testX[test_size:, :, :]\n",
    "Y_valid = testY[test_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5000, 32, 32, 1)\n",
      "Y_train shape: (5000, 10)\n",
      "X_test shape: (5000, 32, 32, 1)\n",
      "Y_test shape: (5000, 10)\n",
      "X_valid shape: (5000, 32, 32, 1)\n",
      "Y_valid shape: (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / 255\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "Y_train = convert_categorical2one_hot(Y_train)\n",
    "X_test = X_test / 255\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "Y_test = convert_categorical2one_hot(Y_test)\n",
    "X_valid = X_valid / 255\n",
    "X_valid = np.expand_dims(X_valid, axis=3)\n",
    "Y_valid = convert_categorical2one_hot(Y_valid)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"Y_valid shape:\", Y_valid.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [],
   "source": [
    "all_layers = [\n",
    "    ConvolutionLayer.initialize(filters=6, kernel_shape=(5, 5, 1), stride=1, padding=2),\n",
    "    # in (N, 28, 28, 1) out (N, 28, 28, 6)\n",
    "    ActivationLayer(),\n",
    "    PoolingLayer(pool_size=(2, 2), stride=2),\n",
    "    ConvolutionLayer.initialize(filters=12, kernel_shape=(5, 5, 6), stride=1, padding=0),\n",
    "    # in (N, 28, 28, 6) out (N, 24, 24, 12)\n",
    "    ActivationLayer(),\n",
    "    PoolingLayer(pool_size=(2, 2), stride=2),\n",
    "    # in (N, 24, 24, 12) out (N, 12, 12, 12)\n",
    "    ConvolutionLayer.initialize(filters=100, kernel_shape=(5, 5, 12), stride=1, padding=0),\n",
    "    # in (N, 12, 12, 12) out (N, 8, 8, 100)\n",
    "    ActivationLayer(),\n",
    "    FlatteningLayer(),\n",
    "    FullyConnectedLayer.initialize(unit_count_previous_layer=100, unit_count_current_layer=10),\n",
    "    SoftmaxLayer(),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [],
   "source": [
    "model = SequentialModel(\n",
    "    layers=all_layers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dims: (32, 32, 32, 1)\n",
      "input_dims: (32, 16, 16, 6)\n",
      "input_dims: (32, 6, 6, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/156 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,400) and (100,10) not aligned: 400 (dim 1) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4385/936888567.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0my_validation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mY_valid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mepoch_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m )\n",
      "\u001B[0;32m/tmp/ipykernel_4385/1450242688.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, x_train, y_train, x_validation, y_validation, epoch_count, batch_size)\u001B[0m\n\u001B[1;32m     14\u001B[0m             \u001B[0my_hat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_batch\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgenerate_batches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtotal\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m                 \u001B[0my_hat_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m                 \u001B[0mactivation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my_hat_batch\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0my_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_4385/1450242688.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0;31m# print(f\"forward layer {layer.__class__.__name__}\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# print(f\"input dim: {activation.shape}\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0mactivation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_previous\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m             \u001B[0;31m# print(f\"output dim: {activation.shape}\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mactivation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_4385/2282344897.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, a_previous)\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma_previous\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0ma_previous\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_previous\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_previous\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbiases\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mda_current\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (32,400) and (100,10) not aligned: 400 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    x_train=X_train,\n",
    "    y_train=Y_train,\n",
    "    x_validation=X_valid,\n",
    "    y_validation=Y_valid,\n",
    "    epoch_count=5,\n",
    "    batch_size=32,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X_test)\n",
    "test_accuracy = softmax_accuracy(y_hat=y_hat_test, y=Y_test)\n",
    "print(f\"Test set accuracy: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}