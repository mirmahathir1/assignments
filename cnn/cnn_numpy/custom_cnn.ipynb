{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist,cifar10\n",
    "from tqdm import tqdm\n",
    "np.random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "weight_scaling = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def categorical_to_one_hot(y: np.array) -> np.array:\n",
    "    one_hot_matrix = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot_matrix[np.arange(y.size), y] = 1\n",
    "    return one_hot_matrix\n",
    "\n",
    "def add_padding_to_image(image, padding):\n",
    "        return np.pad(array=image, pad_width=((0, 0), (padding, padding), (padding, padding), (0, 0)),\n",
    "                      mode='constant')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def generate_mini_batch(x, y, batch_size):\n",
    "    for i in range(0, x.shape[0], batch_size):\n",
    "        yield (x.take(indices=range(\n",
    "            i, min(i + batch_size, x.shape[0])), axis=0),\n",
    "               y.take(indices=range(\n",
    "                   i, min(i + batch_size, y.shape[0])), axis=0)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def get_softmax_accuracy(y_hat, y):\n",
    "    class_index = np.argmax(y_hat, axis=1)\n",
    "    one_hot_matrix = np.zeros_like(y_hat)\n",
    "    one_hot_matrix[np.arange(y_hat.shape[0]), class_index] = 1\n",
    "    return (one_hot_matrix == y).all(axis=1).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "class SequentialModel:\n",
    "    def __init__(self, layers, ):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "    def initialize(self, x_train_sample):\n",
    "        activation_sample = np.expand_dims(x_train_sample, axis=0)\n",
    "        print(\"Configured Network-------\")\n",
    "        for i in range(len(self.layers)):\n",
    "            print(f\"Layer {i}: {self.layers[i].__class__.__name__}\")\n",
    "            self.layers[i].initialize(activation_sample)\n",
    "            print(f\"input dim: {activation_sample.shape}\")\n",
    "            activation_sample = self.layers[i].forward(a_previous=activation_sample)\n",
    "            print(f\"output dim: {activation_sample.shape}\")\n",
    "\n",
    "    def train(self, x_train, y_train, x_validation, y_validation, epoch_count, batch_size):\n",
    "        self.initialize(x_train[0])\n",
    "        for epoch in range(epoch_count):\n",
    "            print(f\"Epoch {epoch + 1}:\")\n",
    "            y_hat = np.zeros_like(y_train)\n",
    "            for index, (x_batch, y_batch) in enumerate(tqdm(generate_mini_batch(x=x_train, y=y_train, batch_size=batch_size),total=int(x_train.shape[0]/batch_size))):\n",
    "                y_hat_batch = self.forward(x=x_batch)\n",
    "                activation = y_hat_batch - y_batch\n",
    "                self.backward(activation)\n",
    "                self.update()\n",
    "                number_start = index * batch_size\n",
    "                number_end = number_start + y_hat_batch.shape[0]\n",
    "                y_hat[number_start: number_end, :] = y_hat_batch\n",
    "\n",
    "            print(\"calculating validation stats...\")\n",
    "\n",
    "            y_hat = self.forward(x_validation)\n",
    "            validation_accuracy = get_softmax_accuracy(y_hat=y_hat, y=y_validation)\n",
    "\n",
    "            # softmax cross entropy\n",
    "            number_examples_in_batch = y_hat.shape[0]\n",
    "            validation_loss = - np.sum(y_validation * np.log(np.clip(y_hat, 1e-20, 1.))) / number_examples_in_batch\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} validation loss {validation_loss} validation acc {validation_accuracy}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activation = x\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward(a_previous=activation)\n",
    "        return activation\n",
    "\n",
    "    def backward(self, x):\n",
    "        activation = x\n",
    "        for layer in reversed(self.layers):\n",
    "            activation = layer.backward(da_current=activation)\n",
    "\n",
    "    def update(self):\n",
    "        # gradient descent\n",
    "        for layer in self.layers:\n",
    "            weights_biases = layer.get_weights()\n",
    "            gradients_weights_biases = layer.get_gradients()\n",
    "            if weights_biases is None or gradients_weights_biases is None:\n",
    "                continue\n",
    "\n",
    "            (weights, biases) = weights_biases\n",
    "            (derivative_weights, derivative_biases) = gradients_weights_biases\n",
    "            layer.set_weights(\n",
    "                weights=weights - self.learning_rate * derivative_weights,\n",
    "                biases=biases - self.learning_rate * derivative_biases\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class PoolingLayer:\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.a = None\n",
    "        self.mask_store = {}\n",
    "    def initialize(self, activation):\n",
    "        pass\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None\n",
    "    def forward(self, a_previous):\n",
    "        # saving the previous activation to calculate the derivative\n",
    "        # of the layer. We only need the shape of the previous activation\n",
    "        self.a = np.array(a_previous, copy=True)\n",
    "        number_example_in_batch, height_input, width_input, channel_count = a_previous.shape\n",
    "\n",
    "        # calculate height and width of output of pooling layer\n",
    "        # and create output matrix for pooling layer\n",
    "        height_output = 1 + (height_input - self.pool_size) // self.stride\n",
    "        width_output = 1 + (width_input - self.pool_size) // self.stride\n",
    "        pooling_layer_output = np.zeros((number_example_in_batch, height_output, width_output, channel_count))\n",
    "\n",
    "        # iterate though all cells of output of pooling layer\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                # get the bounding box of input activations where the cell (i,j)\n",
    "                # of pooling layer will search for max value\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + self.pool_size\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + self.pool_size\n",
    "                a_previous_slice = a_previous[:, height_start:height_end, width_start:width_end, :]\n",
    "\n",
    "                # save the index positions from which the max value was collected\n",
    "                self.save_mask_to_store(x=a_previous_slice, coordinates=(i, j))\n",
    "\n",
    "                # update the output cell (i,j) of the pooling layer\n",
    "                pooling_layer_output[:, i, j, :] = np.max(a_previous_slice, axis=(1, 2))\n",
    "        return pooling_layer_output\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        # create the output matrix by the shape of the previous activation\n",
    "        # passed through this layer\n",
    "        output = np.zeros_like(self.a)\n",
    "        _, height_output, width_output, _ = da_current.shape\n",
    "\n",
    "        # iterate through all the empty cells of the output matrix\n",
    "        # and add the value of the da_current cell which is in the position\n",
    "        # of the coordinates of the max value selected in the max pooling\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                # the cells we are populating of the output matrix\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + self.pool_size\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + self.pool_size\n",
    "\n",
    "                # use the mask to add values from the coordinates which\n",
    "                # possessed the max values\n",
    "                output[:, height_start:height_end, width_start:width_end, :] += da_current[:, i:i + 1, j:j + 1, :] * \\\n",
    "                                                                                self.mask_store[(i, j)]\n",
    "        return output\n",
    "\n",
    "    def save_mask_to_store(self, x, coordinates):\n",
    "        # create a max with the same shape as the slice of the activation used\n",
    "        # for max pooling\n",
    "        mask = np.zeros_like(x)\n",
    "        number_example_in_batch, height, width, channel_count = x.shape\n",
    "        x = x.reshape(number_example_in_batch, height * width, channel_count)\n",
    "        index = np.argmax(x, axis=1)\n",
    "        number_index, channel_index = np.indices((number_example_in_batch, channel_count))\n",
    "        mask.reshape(number_example_in_batch, height * width, channel_count)[number_index, index, channel_index] = 1\n",
    "        self.mask_store[coordinates] = mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        self.shape = ()\n",
    "    def initialize(self, activation):\n",
    "        pass\n",
    "    def forward(self, a_previous):\n",
    "        self.shape = a_previous.shape\n",
    "        return np.ravel(a_previous).reshape(a_previous.shape[0], -1)\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        return da_current.reshape(self.shape)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, unit_count_current_layer):\n",
    "        self.unit_count_current_layer = unit_count_current_layer\n",
    "        #\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.derivative_weights = None\n",
    "        self.derivative_biases = None\n",
    "        self.a_previous = None\n",
    "\n",
    "    def initialize(self, activation):\n",
    "        self.weights = np.random.randn(self.unit_count_current_layer, activation.shape[1]) * weight_scaling\n",
    "        self.biases = np.random.randn(1, self.unit_count_current_layer) * weight_scaling\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def get_gradients(self):\n",
    "        if self.derivative_weights is None or self.derivative_biases is None:\n",
    "            return None\n",
    "        return self.derivative_weights, self.derivative_biases\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        self.a_previous = np.array(a_previous, copy=True)\n",
    "        return np.dot(a_previous, self.weights.T) + self.biases\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        number_example_in_batch = self.a_previous.shape[0]\n",
    "        self.derivative_weights = np.dot(da_current.T, self.a_previous) / number_example_in_batch\n",
    "        self.derivative_biases = np.sum(da_current, axis=0, keepdims=True) / number_example_in_batch\n",
    "        return np.dot(da_current, self.weights)\n",
    "\n",
    "    def set_weights(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, filters, kernel_shape, padding, stride):\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.filters = filters\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.derivative_weights = None\n",
    "        self.derivative_biases = None\n",
    "        self.a_previous = None\n",
    "\n",
    "    def initialize(self, activation):\n",
    "        self.weights = np.random.randn(self.kernel_shape,self.kernel_shape,activation.shape[3], self.filters) * weight_scaling\n",
    "        self.biases = np.random.randn(self.filters) * weight_scaling\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def set_weights(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def get_gradients(self):\n",
    "        if self.derivative_weights is None or self.derivative_biases is None:\n",
    "            return None\n",
    "        return self.derivative_weights, self.derivative_biases\n",
    "\n",
    "    def forward(self, a_previous):\n",
    "        self.a_previous = np.array(a_previous, copy=True)\n",
    "        # output_shape = self.calculate_output_dims(input_dims=a_previous.shape)\n",
    "\n",
    "        # calculating output shape of convolution\n",
    "        number_examples_in_batch, height_input, width_of_input, _ = a_previous.shape\n",
    "        height_filter, width_filter, _, number_filters = self.weights.shape\n",
    "\n",
    "        height_output = (height_input - height_filter + 2 * self.padding) // self.stride + 1\n",
    "        width_output = (width_of_input - width_filter + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        height_filter, width_filter, _, number_filter = self.weights.shape\n",
    "\n",
    "        a_previous_padding = add_padding_to_image(image=a_previous, padding=self.padding)\n",
    "        output = np.zeros((number_examples_in_batch, height_output, width_output, number_filters))\n",
    "\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + height_filter\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + width_filter\n",
    "                output[:, i, j, :] = np.sum(a_previous_padding[:, height_start:height_end, width_start:width_end, :, np.newaxis] * self.weights[np.newaxis,:, :,:],axis=(1, 2, 3))\n",
    "        return output + self.biases\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        _, height_output, width_output, _ = da_current.shape\n",
    "        number_example_in_batch, height_input, width_input, _ = self.a_previous.shape\n",
    "        height_filter, width_filter, _, _ = self.weights.shape\n",
    "        # padding = np.full((4,), self.padding)\n",
    "        a_previous_padding = add_padding_to_image(image=self.a_previous, padding=self.padding)\n",
    "        output = np.zeros_like(a_previous_padding)\n",
    "        self.derivative_biases = da_current.sum(axis=(0, 1, 2)) / number_example_in_batch\n",
    "        self.derivative_weights = np.zeros_like(self.weights)\n",
    "        for i in range(height_output):\n",
    "            for j in range(width_output):\n",
    "                height_start = i * self.stride\n",
    "                height_end = height_start + height_filter\n",
    "                width_start = j * self.stride\n",
    "                width_end = width_start + width_filter\n",
    "                output[:, height_start:height_end, width_start: width_end, :] += np.sum(\n",
    "                    self.weights[np.newaxis, :, :, :, :] *\n",
    "                    da_current[:, i:i + 1, j:j + 1, np.newaxis, :],\n",
    "                    axis=4\n",
    "                )\n",
    "                self.derivative_weights += np.sum(\n",
    "                    a_previous_padding[:, height_start: height_end, width_start: width_end, :, np.newaxis] *\n",
    "                    da_current[:, i:i + 1, j:j + 1, np.newaxis, :],\n",
    "                    axis=0\n",
    "                )\n",
    "        self.derivative_weights /= number_example_in_batch\n",
    "        return output[:, self.padding:self.padding + height_input, self.padding:self.padding + width_input, :]\n",
    "\n",
    "    def calculate_output_dims(self, input_dims):\n",
    "        number_examples_in_batch, height_input, width_of_input, _ = input_dims\n",
    "        height_filter, width_filter, _, number_filters = self.weights.shape\n",
    "\n",
    "        height_output = (height_input - height_filter + 2 * self.padding) // self.stride + 1\n",
    "        width_output = (width_of_input - width_filter + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        return number_examples_in_batch, height_output, width_output, number_filters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    def initialize(self, activation):\n",
    "        pass\n",
    "    def forward(self, a_previous):\n",
    "        self.z = np.maximum(0, a_previous)\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        derivative_z = np.array(da_current, copy=True)\n",
    "        derivative_z[self.z <= 0] = 0\n",
    "        return derivative_z\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    def initialize(self, activation):\n",
    "        pass\n",
    "    def forward(self, a_previous):\n",
    "        exponential = np.exp(a_previous - a_previous.max(axis=1, keepdims=True))\n",
    "        self.z = exponential / np.sum(exponential, axis=1, keepdims=True)\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, da_current):\n",
    "        return da_current\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    def get_gradients(self):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# number of samples in the train data set\n",
    "N_TRAIN_SAMPLES = 5000\n",
    "dataset = \"mnist\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape: (60000, 28, 28)\n",
      "trainY shape: (60000,)\n",
      "testX shape: (10000, 28, 28)\n",
      "testY shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = None, None, None, None\n",
    "if dataset == \"mnist\":\n",
    "    ((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "elif dataset == \"cifar10\":\n",
    "    ((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "    trainX = rgb2gray(trainX).astype(int)\n",
    "    testX = rgb2gray(testX)\n",
    "    trainY = trainY.reshape((trainY.shape[0],))\n",
    "    testY = testY.reshape((testY.shape[0],))\n",
    "\n",
    "print(\"trainX shape:\", trainX.shape)\n",
    "print(\"trainY shape:\", trainY.shape)\n",
    "print(\"testX shape:\", testX.shape)\n",
    "print(\"testY shape:\", testY.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "X_train = trainX[:N_TRAIN_SAMPLES, :, :]\n",
    "Y_train = trainY[:N_TRAIN_SAMPLES]\n",
    "\n",
    "test_size = int(testX.shape[0]/2)\n",
    "X_test = testX[:test_size, :, :]\n",
    "Y_test = testY[:test_size]\n",
    "\n",
    "X_valid = testX[test_size:, :, :]\n",
    "Y_valid = testY[test_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5000, 28, 28, 1)\n",
      "Y_train shape: (5000, 10)\n",
      "X_test shape: (5000, 28, 28, 1)\n",
      "Y_test shape: (5000, 10)\n",
      "X_valid shape: (5000, 28, 28, 1)\n",
      "Y_valid shape: (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / 255\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "Y_train = categorical_to_one_hot(Y_train)\n",
    "X_test = X_test / 255\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "Y_test = categorical_to_one_hot(Y_test)\n",
    "X_valid = X_valid / 255\n",
    "X_valid = np.expand_dims(X_valid, axis=3)\n",
    "Y_valid = categorical_to_one_hot(Y_valid)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"Y_valid shape:\", Y_valid.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "all_layers = [\n",
    "    ConvolutionLayer(filters=6, kernel_shape=5, stride=1, padding=2),\n",
    "    ActivationLayer(),\n",
    "    PoolingLayer(pool_size=2, stride=2),\n",
    "    ConvolutionLayer(filters=12, kernel_shape=5, stride=1, padding=0),\n",
    "    ActivationLayer(),\n",
    "    PoolingLayer(pool_size=2, stride=2),\n",
    "    ConvolutionLayer(filters=100, kernel_shape=5, stride=1, padding=0),\n",
    "    ActivationLayer(),\n",
    "    FlatteningLayer(),\n",
    "    FullyConnectedLayer(unit_count_current_layer=10),\n",
    "    SoftmaxLayer(),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "model = SequentialModel(\n",
    "    layers=all_layers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Network-------\n",
      "Layer 0: ConvolutionLayer\n",
      "input dim: (1, 28, 28, 1)\n",
      "output dim: (1, 28, 28, 6)\n",
      "Layer 1: ActivationLayer\n",
      "input dim: (1, 28, 28, 6)\n",
      "output dim: (1, 28, 28, 6)\n",
      "Layer 2: PoolingLayer\n",
      "input dim: (1, 28, 28, 6)\n",
      "output dim: (1, 14, 14, 6)\n",
      "Layer 3: ConvolutionLayer\n",
      "input dim: (1, 14, 14, 6)\n",
      "output dim: (1, 10, 10, 12)\n",
      "Layer 4: ActivationLayer\n",
      "input dim: (1, 10, 10, 12)\n",
      "output dim: (1, 10, 10, 12)\n",
      "Layer 5: PoolingLayer\n",
      "input dim: (1, 10, 10, 12)\n",
      "output dim: (1, 5, 5, 12)\n",
      "Layer 6: ConvolutionLayer\n",
      "input dim: (1, 5, 5, 12)\n",
      "output dim: (1, 1, 1, 100)\n",
      "Layer 7: ActivationLayer\n",
      "input dim: (1, 1, 1, 100)\n",
      "output dim: (1, 1, 1, 100)\n",
      "Layer 8: FlatteningLayer\n",
      "input dim: (1, 1, 1, 100)\n",
      "output dim: (1, 100)\n",
      "Layer 9: FullyConnectedLayer\n",
      "input dim: (1, 100)\n",
      "output dim: (1, 10)\n",
      "Layer 10: SoftmaxLayer\n",
      "input dim: (1, 10)\n",
      "output dim: (1, 10)\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 27/156 [00:05<00:24,  5.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_8941/2436442831.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0my_validation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mY_valid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mepoch_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8941/727569981.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, x_train, y_train, x_validation, y_validation, epoch_count, batch_size)\u001B[0m\n\u001B[1;32m     22\u001B[0m                 \u001B[0my_hat_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m                 \u001B[0mactivation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my_hat_batch\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0my_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m                 \u001B[0mnumber_start\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8941/727569981.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     52\u001B[0m         \u001B[0mactivation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mreversed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m             \u001B[0mactivation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mda_current\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8941/2023184001.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, da_current)\u001B[0m\n\u001B[1;32m     70\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnewaxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m                     \u001B[0mda_current\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mj\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnewaxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m                     \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     73\u001B[0m                 )\n\u001B[1;32m     74\u001B[0m                 self.derivative_weights += np.sum(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    x_train=X_train,\n",
    "    y_train=Y_train,\n",
    "    x_validation=X_valid,\n",
    "    y_validation=Y_valid,\n",
    "    epoch_count=5,\n",
    "    batch_size=32,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X_test)\n",
    "test_accuracy = get_softmax_accuracy(y_hat=y_hat_test, y=Y_test)\n",
    "print(f\"Test set accuracy: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}